{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and visualize a model in Tensorflow - Part 4: Inspecting the model\n",
    "\n",
    "Neural networks have been widely critized because of the lack of interpretation of their internal parameters. In this notebook we will present some techniques to log and visualize the model behaviour during training.\n",
    "\n",
    "The lack of interpretability leads, among other thigns, to make neural models error prone. While this is true, we still have some tools to try to debug our network and to understand what the model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a numpy keyed structure\n",
    "newsgroups = np.load('./resources/newsgroup.npz')\n",
    "\n",
    "# Define the batch size and the number of labels\n",
    "batch_size = 100\n",
    "num_classes = newsgroups['labels'].shape[0]\n",
    "\n",
    "def dataset_input_fn(dataset):\n",
    "    \"\"\"\n",
    "    Creates an input function using the `numpy_input_fn` method from\n",
    "    tensorflow, based on the dataset we want to use.\n",
    "    \n",
    "    Args:\n",
    "        dataset: String that represents the dataset (should be `train` or `test`)\n",
    "    \n",
    "    Returns:\n",
    "        An `numpy_input_fn` function to feed to an estimator\n",
    "    \"\"\"\n",
    "    assert dataset in ('train', 'test'), \"The selected dataset should be `train` or `test`\"\n",
    "    \n",
    "    return tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'input_data': newsgroups['%s_data' % dataset]},\n",
    "        y=newsgroups['%s_target' % dataset],\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=1 if dataset == 'test' else None,\n",
    "        shuffle=dataset == 'train'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The easiest way of logging values\n",
    "\n",
    "If you only need to see some numerical values during training, you can print them in the console (or notebook in this case).\n",
    "\n",
    "To add any operation that is performed inside the training cycle, the `Estimator.train` method provides hooks. Hooks, which are formally instances of subclasses of `SessionRunHook`, will be called after each epoch iteration to perform the operation you want, depending on the type of hook. In this particulaer case, the `LoggingTensorHook` will print in console the tensors we give as parameters, and we can personalize after how many iterations the print will occur. This will also work for the evaluate and predict methods.\n",
    "\n",
    "To try the logging, just run the above training phase with the model we presented on the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up logging for predictions\n",
    "# Log the values in the \"Softmax\" tensor with label \"probabilities\"\n",
    "tensors_to_log = {'probabilities': 'softmax_tensor'}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "# Train the model\n",
    "mlp_classifier.train(\n",
    "    input_fn=dataset_input_fn('train'),\n",
    "    steps=2000,\n",
    "    hooks=[logging_hook]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "There is a limit to what we can print and interpret on console. Tensorflow comes with its own (and very complete) visualization tool: Tensorboard. In the rest of this tutorial, we will explain how to use Tensorboard to log scalar values like metrics of performance, histogram values like the activation of the cells in each network layer. In the next notebook we will see how to plot and inspect embeddings to show how the document embeddings relate to each other.\n",
    "\n",
    "Tensorboard is based on operations called summaries which record the tensor variable to log. Unlike the previous example, summaries, as all operations, must be compiled along with the model in order to be included in the execution graph. There is a summary operation for each type of data that we want to log: scalars, tensors (histogram or tensor), audio, images and text.\n",
    "\n",
    "In any tensorflow code where we want to save variables for Tensorboard, we have to add some code with the following structure:\n",
    "\n",
    "```\n",
    "    # The definition of your variables\n",
    "    ...\n",
    "    # The summary operations\n",
    "    tf.summary.histogram('softmax_tensor', probabilities_tensor)\n",
    "    tf.summary.scalar('loss', loss_value)\n",
    "    \n",
    "    # The merge operation\n",
    "    tf.summary.merge_all()\n",
    "    \n",
    "    # The write operation\n",
    "    ...\n",
    "```\n",
    "\n",
    "The `summary.histogram` and `summary.scalar` will evaluate the value of the variable at that point during the execution of the graph. Then, the `summary.merge_all` takes all the summary operations added up to that moment and creates a single output with all the information, so the result can be written to disk only once.\n",
    "\n",
    "Now, for older versions of tensorflow or if you are not using Estimators, the write operation uses the `summary.FileWriter` class to write your data. On the other hand, the Estimator wraps this task into a special Hook for summary operations called `SummarySaverHook`.\n",
    "\n",
    "In the following cell we have the same model structure as before (with less comments) and we add the summary operations to the graph, and finally the summary hook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_data, mode):\n",
    "    \"\"\"Creates the model layers.\n",
    "    \n",
    "    Args:\n",
    "        input_data: a Tensor with shape [batch_size, feature_size]\n",
    "    \n",
    "    Returns:\n",
    "        The logits of the output layer.\"\"\"\n",
    "    hidden1 = tf.layers.dense(inputs=input_data, units=250, activation=tf.nn.relu,\n",
    "                              name='hidden_layer_1')\n",
    "    hidden2 = tf.layers.dense(inputs=hidden1, units=100, activation=tf.nn.relu,\n",
    "                              name='hidden_layer_2')\n",
    "    dropout = tf.layers.dropout(inputs=hidden2, rate=0.4,\n",
    "                                training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "    logits = tf.layers.dense(inputs=dropout, units=num_classes, name='logits')\n",
    "\n",
    "    return (logits)\n",
    "\n",
    "def mlp_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for MLP.\n",
    "    \n",
    "    Args:\n",
    "        features: a dictionary where the values are input tensors with shape\n",
    "            [batch_size, feature_size]\n",
    "        labels: a tensor with shape [batch_size]\n",
    "        mode: a constant, one of `tf.estimator.ModeKeys.`\n",
    "    \n",
    "    Returns:\n",
    "        An instance of ´tf.estimator.EstimatorSpec´.\n",
    "    \"\"\"\n",
    "    logits = build_model(features['input_data'], mode)\n",
    "\n",
    "    predictions = {\n",
    "        'classes': tf.argmax(input=logits, axis=1),\n",
    "        'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
    "    }\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Add the summary operation to log the tensor with the predictions\n",
    "    tf.summary.histogram('softmax_tensor', predictions['probabilities'])\n",
    "\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=num_classes)\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "      onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "    accuracy_op = tf.metrics.accuracy(labels=labels, predictions=predictions['classes'], name='accuracy')\n",
    "    # Add the summary operation to log the value of the accuracy\n",
    "    tf.summary.scalar('accuracy', accuracy_op[1])\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    summary_hook = tf.train.SummarySaverHook(save_steps=100, summary_op=summary_op)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "        train_op = optimizer.minimize(loss=loss,\n",
    "                                      global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op,\n",
    "                                          training_hooks=[summary_hook])\n",
    "\n",
    "    eval_metric_ops = {'accuracy': accuracy_op}\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops,\n",
    "                                      evaluation_hooks=[summary_hook])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the Estimator as before with the summary operations compiled into the graph. Note that, as we have a different graph, we have to use a new model_dir or it would fail when loading the previous checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_master': '', '_log_step_count_steps': 100, '_num_ps_replicas': 0, '_tf_random_seed': None, '_service': None, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_task_id': 0, '_session_config': None, '_save_summary_steps': 100, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_task_type': 'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcc4fda3208>, '_model_dir': '20news_mlp_summaries', '_is_chief': True, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "mlp_classifier = tf.estimator.Estimator(\n",
    "    model_fn=mlp_model_fn, model_dir='20news_mlp_summaries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into 20news_mlp_summaries/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.14883112907, step = 1\n",
      "INFO:tensorflow:global_step/sec: 166.72\n",
      "INFO:tensorflow:loss = 1.22570002079, step = 101 (0.601 sec)\n",
      "INFO:tensorflow:global_step/sec: 183.863\n",
      "INFO:tensorflow:loss = 1.09616887569, step = 201 (0.544 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.621\n",
      "INFO:tensorflow:loss = 0.891860187054, step = 301 (0.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.302\n",
      "INFO:tensorflow:loss = 0.953999936581, step = 401 (0.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.314\n",
      "INFO:tensorflow:loss = 0.981770038605, step = 501 (0.570 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.851\n",
      "INFO:tensorflow:loss = 0.726292848587, step = 601 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.98\n",
      "INFO:tensorflow:loss = 1.12570285797, step = 701 (0.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.108\n",
      "INFO:tensorflow:loss = 0.795264601707, step = 801 (0.521 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.37\n",
      "INFO:tensorflow:loss = 0.743711471558, step = 901 (0.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.592\n",
      "INFO:tensorflow:loss = 0.7079423666, step = 1001 (0.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.434\n",
      "INFO:tensorflow:loss = 0.84659487009, step = 1101 (0.520 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.261\n",
      "INFO:tensorflow:loss = 0.771671533585, step = 1201 (0.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.551\n",
      "INFO:tensorflow:loss = 0.627817213535, step = 1301 (0.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.669\n",
      "INFO:tensorflow:loss = 0.541033506393, step = 1401 (0.561 sec)\n",
      "INFO:tensorflow:global_step/sec: 186.234\n",
      "INFO:tensorflow:loss = 0.553482055664, step = 1501 (0.536 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.881\n",
      "INFO:tensorflow:loss = 0.568674981594, step = 1601 (0.527 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.344\n",
      "INFO:tensorflow:loss = 0.673816084862, step = 1701 (0.528 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.832\n",
      "INFO:tensorflow:loss = 0.638634502888, step = 1801 (0.516 sec)\n",
      "INFO:tensorflow:global_step/sec: 189.258\n",
      "INFO:tensorflow:loss = 0.4653865695, step = 1901 (0.528 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into 20news_mlp_summaries/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.621620893478.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7fcc00405978>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_classifier.train(input_fn=dataset_input_fn('train'), steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-11-15-15:08:42\n",
      "INFO:tensorflow:Restoring parameters from 20news_mlp_summaries/model.ckpt-2000\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-15-15:08:42\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.74137, global_step = 2000, loss = 0.910075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.74137014, 'global_step': 2000, 'loss': 0.91007513}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_classifier.evaluate(input_fn=dataset_input_fn('test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The next thing to do is to go to the Tensorboard dashboard in the model directory and inspect the obtained values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple iterations\n",
    "\n",
    "If we run the same experiment as before several times, the model will be restored from the last checkpoint and the training will recommence using the current weights and biases. Even more, if you don't use Estimators the results of different runs are stored in the same folder and coexists in a mess of metric values.\n",
    "\n",
    "We actually want to compare several runs of the same experiment, or perhaps compare the performance of several classifiers in the same graph. For that, we will change the structure of the directories and Tensorboard will organize and show the results accordingly. We will use an experiment counter to keep track of how many iterations we have done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_COUNTER = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_master': '', '_log_step_count_steps': 100, '_num_ps_replicas': 0, '_tf_random_seed': None, '_service': None, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_task_id': 0, '_session_config': None, '_save_summary_steps': 100, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_task_type': 'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcc105ecd68>, '_model_dir': '20news_mlp_summaries/iter1', '_is_chief': True, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into 20news_mlp_summaries/iter1/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.13280391693, step = 1\n",
      "INFO:tensorflow:global_step/sec: 170.08\n",
      "INFO:tensorflow:loss = 1.52901899815, step = 101 (0.591 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.217\n",
      "INFO:tensorflow:loss = 1.4076641798, step = 201 (0.569 sec)\n",
      "INFO:tensorflow:global_step/sec: 158.987\n",
      "INFO:tensorflow:loss = 1.16960251331, step = 301 (0.629 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.142\n",
      "INFO:tensorflow:loss = 1.19291830063, step = 401 (0.571 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.322\n",
      "INFO:tensorflow:loss = 0.755596339703, step = 501 (0.507 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.637\n",
      "INFO:tensorflow:loss = 0.990729391575, step = 601 (0.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 133.915\n",
      "INFO:tensorflow:loss = 0.768337011337, step = 701 (0.749 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.198\n",
      "INFO:tensorflow:loss = 0.978021383286, step = 801 (0.874 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.033\n",
      "INFO:tensorflow:loss = 1.03076457977, step = 901 (0.870 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.617\n",
      "INFO:tensorflow:loss = 0.802337408066, step = 1001 (0.864 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.619\n",
      "INFO:tensorflow:loss = 0.832582235336, step = 1101 (0.843 sec)\n",
      "INFO:tensorflow:global_step/sec: 191.402\n",
      "INFO:tensorflow:loss = 0.606909573078, step = 1201 (0.522 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.924\n",
      "INFO:tensorflow:loss = 0.671876847744, step = 1301 (0.578 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.517\n",
      "INFO:tensorflow:loss = 0.649886906147, step = 1401 (0.608 sec)\n",
      "INFO:tensorflow:global_step/sec: 162.53\n",
      "INFO:tensorflow:loss = 0.644362330437, step = 1501 (0.615 sec)\n",
      "INFO:tensorflow:global_step/sec: 135.089\n",
      "INFO:tensorflow:loss = 0.532850623131, step = 1601 (0.742 sec)\n",
      "INFO:tensorflow:global_step/sec: 116.023\n",
      "INFO:tensorflow:loss = 0.544606268406, step = 1701 (0.860 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.667\n",
      "INFO:tensorflow:loss = 0.633141815662, step = 1801 (0.576 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.127\n",
      "INFO:tensorflow:loss = 0.508603334427, step = 1901 (0.500 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into 20news_mlp_summaries/iter1/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.51029998064.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "mlp_classifier = tf.estimator.Estimator(\n",
    "    model_fn=mlp_model_fn,\n",
    "    model_dir=os.path.join('20news_mlp_summaries', 'iter{}'.format(EXPERIMENT_COUNTER)))\n",
    "mlp_classifier.train(input_fn=dataset_input_fn('train'), steps=2000)\n",
    "EXPERIMENT_COUNTER += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrib: Monitoring performance while training\n",
    "\n",
    "We know so far how to visualize the metrics related to training, but it is more interesting to compare the performance of the classifier in the validation dataset. Furthermore, we would like to stop the training if the validation performance drops too much.\n",
    "\n",
    "In the contrib.learn module of tensorflow we found an `Experiment` class that will run the train and evaluation cycle for us. Even if this not hard to implement using a for loop, we recommend to use the functions provided by tensorflow as they support training on multiple servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_master': '', '_log_step_count_steps': 100, '_num_ps_replicas': 0, '_tf_random_seed': None, '_service': None, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_task_id': 0, '_session_config': None, '_save_summary_steps': 100, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_task_type': 'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcbe8d89c50>, '_model_dir': '20news_mlp_model_summaries/iter3', '_is_chief': True, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:Experiment.continuous_train_and_eval (from tensorflow.contrib.learn.python.learn.experiment) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO:tensorflow:Training model for 100 steps\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into 20news_mlp_model_summaries/iter3/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.05844807625, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into 20news_mlp_model_summaries/iter3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.68481183052.\n",
      "INFO:tensorflow:Evaluating model now.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-15-15:20:14\n",
      "INFO:tensorflow:Restoring parameters from 20news_mlp_model_summaries/iter3/model.ckpt-100\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-15-15:20:14\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.165, global_step = 100, loss = 2.68826\n",
      "INFO:tensorflow:Training model for 100 steps\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from 20news_mlp_model_summaries/iter3/model.ckpt-100\n",
      "INFO:tensorflow:Saving checkpoints for 101 into 20news_mlp_model_summaries/iter3/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.63216042519, step = 101\n",
      "INFO:tensorflow:Saving checkpoints for 200 into 20news_mlp_model_summaries/iter3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.28964567184.\n",
      "INFO:tensorflow:Evaluating model now.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-15-15:20:16\n",
      "INFO:tensorflow:Restoring parameters from 20news_mlp_model_summaries/iter3/model.ckpt-200\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-15-15:20:16\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.24, global_step = 200, loss = 2.39362\n",
      "INFO:tensorflow:Training model for 100 steps\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from 20news_mlp_model_summaries/iter3/model.ckpt-200\n",
      "INFO:tensorflow:Saving checkpoints for 201 into 20news_mlp_model_summaries/iter3/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.60118317604, step = 201\n",
      "INFO:tensorflow:Saving checkpoints for 300 into 20news_mlp_model_summaries/iter3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.29096794128.\n",
      "INFO:tensorflow:Evaluating model now.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-15-15:20:18\n",
      "INFO:tensorflow:Restoring parameters from 20news_mlp_model_summaries/iter3/model.ckpt-300\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-15-15:20:18\n",
      "INFO:tensorflow:Saving dict for global step 300: accuracy = 0.27, global_step = 300, loss = 2.14859\n",
      "INFO:tensorflow:Training model for 100 steps\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from 20news_mlp_model_summaries/iter3/model.ckpt-300\n",
      "INFO:tensorflow:Saving checkpoints for 301 into 20news_mlp_model_summaries/iter3/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.3066637516, step = 301\n",
      "INFO:tensorflow:Saving checkpoints for 400 into 20news_mlp_model_summaries/iter3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.28885769844.\n",
      "INFO:tensorflow:Evaluating model now.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-15-15:20:19\n",
      "INFO:tensorflow:Restoring parameters from 20news_mlp_model_summaries/iter3/model.ckpt-400\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-15-15:20:20\n",
      "INFO:tensorflow:Saving dict for global step 400: accuracy = 0.36, global_step = 400, loss = 1.94317\n",
      "INFO:tensorflow:Training model for 100 steps\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from 20news_mlp_model_summaries/iter3/model.ckpt-400\n",
      "INFO:tensorflow:Saving checkpoints for 401 into 20news_mlp_model_summaries/iter3/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.03677248955, step = 401\n",
      "INFO:tensorflow:Saving checkpoints for 500 into 20news_mlp_model_summaries/iter3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.94808888435.\n",
      "INFO:tensorflow:Evaluating model now.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-15-15:20:21\n",
      "INFO:tensorflow:Restoring parameters from 20news_mlp_model_summaries/iter3/model.ckpt-500\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-15-15:20:21\n",
      "INFO:tensorflow:Saving dict for global step 500: accuracy = 0.395, global_step = 500, loss = 1.7098\n",
      "INFO:tensorflow:Training model for 100 steps\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from 20news_mlp_model_summaries/iter3/model.ckpt-500\n",
      "INFO:tensorflow:Saving checkpoints for 501 into 20news_mlp_model_summaries/iter3/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.66131174564, step = 501\n",
      "INFO:tensorflow:Saving checkpoints for 600 into 20news_mlp_model_summaries/iter3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.01225137711.\n",
      "INFO:tensorflow:Evaluating model now.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-15-15:20:23\n",
      "INFO:tensorflow:Restoring parameters from 20news_mlp_model_summaries/iter3/model.ckpt-600\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-15-15:20:23\n",
      "INFO:tensorflow:Saving dict for global step 600: accuracy = 0.5, global_step = 600, loss = 1.53732\n",
      "INFO:tensorflow:Training model for 100 steps\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from 20news_mlp_model_summaries/iter3/model.ckpt-600\n",
      "INFO:tensorflow:Saving checkpoints for 601 into 20news_mlp_model_summaries/iter3/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.83180272579, step = 601\n",
      "INFO:tensorflow:Saving checkpoints for 700 into 20news_mlp_model_summaries/iter3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.12139463425.\n",
      "INFO:tensorflow:Evaluating model now.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-15-15:20:25\n",
      "INFO:tensorflow:Restoring parameters from 20news_mlp_model_summaries/iter3/model.ckpt-700\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-15-15:20:25\n",
      "INFO:tensorflow:Saving dict for global step 700: accuracy = 0.59, global_step = 700, loss = 1.36072\n",
      "INFO:tensorflow:Training model for 100 steps\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from 20news_mlp_model_summaries/iter3/model.ckpt-700\n",
      "INFO:tensorflow:Saving checkpoints for 701 into 20news_mlp_model_summaries/iter3/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.68244707584, step = 701\n",
      "INFO:tensorflow:Saving checkpoints for 800 into 20news_mlp_model_summaries/iter3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.44576120377.\n",
      "INFO:tensorflow:Evaluating model now.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-15-15:20:26\n",
      "INFO:tensorflow:Restoring parameters from 20news_mlp_model_summaries/iter3/model.ckpt-800\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-15-15:20:27\n",
      "INFO:tensorflow:Saving dict for global step 800: accuracy = 0.625, global_step = 800, loss = 1.20557\n",
      "INFO:tensorflow:Training model for 100 steps\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from 20news_mlp_model_summaries/iter3/model.ckpt-800\n",
      "INFO:tensorflow:Saving checkpoints for 801 into 20news_mlp_model_summaries/iter3/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.38161897659, step = 801\n",
      "INFO:tensorflow:Saving checkpoints for 900 into 20news_mlp_model_summaries/iter3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.44885265827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluating model now.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-15-15:20:28\n",
      "INFO:tensorflow:Restoring parameters from 20news_mlp_model_summaries/iter3/model.ckpt-900\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-15-15:20:28\n",
      "INFO:tensorflow:Saving dict for global step 900: accuracy = 0.695, global_step = 900, loss = 1.04542\n",
      "INFO:tensorflow:Training model for 100 steps\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from 20news_mlp_model_summaries/iter3/model.ckpt-900\n",
      "INFO:tensorflow:Saving checkpoints for 901 into 20news_mlp_model_summaries/iter3/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.37891507149, step = 901\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into 20news_mlp_model_summaries/iter3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.36538612843.\n",
      "INFO:tensorflow:Evaluating model now.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-15-15:20:30\n",
      "INFO:tensorflow:Restoring parameters from 20news_mlp_model_summaries/iter3/model.ckpt-1000\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-15-15:20:30\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.675, global_step = 1000, loss = 1.0218\n",
      "INFO:tensorflow:Stop training model as max steps reached\n"
     ]
    }
   ],
   "source": [
    "mlp_classifier = tf.estimator.Estimator(\n",
    "    model_fn=mlp_model_fn,\n",
    "    model_dir=os.path.join('20news_mlp_model_summaries', 'iter{}'.format(EXPERIMENT_COUNTER)))\n",
    "\n",
    "experiment = tf.contrib.learn.Experiment(\n",
    "    mlp_classifier,\n",
    "    train_input_fn=train_input_fn,\n",
    "    eval_input_fn=eval_input_fn,\n",
    "    train_steps=1000,\n",
    "    train_steps_per_iteration=100\n",
    "    )\n",
    "experiment.continuous_train_and_eval()\n",
    "\n",
    "EXPERIMENT_COUNTER += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity\n",
    "\n",
    " 1. Use Tensoboard to visualize, the precision and recall of several runs and several models in the same graph.\n",
    " 2. Create an early stop training cycle using the `contrib.learn.Experiment` class or the `tf.estimator.train_and_evaluate` function. This cycle must train the model until the performance on the test dataset drops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
