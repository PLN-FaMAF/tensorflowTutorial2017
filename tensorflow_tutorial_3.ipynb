{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and visualize a model in Tensorflow - Part 3: Advanced Tensorflow\n",
    "\n",
    "Now that we've trained a linear model from the dataset with an estimator already implemented by Tensorflow, it's time to have some more complexity using a neural network. This part of the tutorial covers how create a tensorflow custom estimator to use as a multilayer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "Like in the previous part, we need to load the 20 newsgroups dataset and define the input functions to feed the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset into a numpy keyed structure\n",
    "newsgroups = np.load('./resources/newsgroup.npz')\n",
    "\n",
    "# Define the batch size and the number of labels\n",
    "batch_size = 100\n",
    "num_classes = newsgroups['labels'].shape[0]\n",
    "\n",
    "def dataset_input_fn(dataset):\n",
    "    \"\"\"\n",
    "    Creates an input function using the `numpy_input_fn` method from\n",
    "    tensorflow, based on the dataset we want to use.\n",
    "    \n",
    "    Args:\n",
    "        dataset: String that represents the dataset (should be `train` or `test`)\n",
    "    \n",
    "    Returns:\n",
    "        An `numpy_input_fn` function to feed to an estimator\n",
    "    \"\"\"\n",
    "    assert dataset in ('train', 'test'), \"The selected dataset should be `train` or `test`\"\n",
    "    \n",
    "    return tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'input_data': newsgroups['%s_data' % dataset]},\n",
    "        y=newsgroups['%s_target' % dataset],\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=1 if dataset == 'test' else None,\n",
    "        shuffle=dataset == 'train'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buiding the MLP model\n",
    "\n",
    "Now the dataset has been processed, we can build the MLP model using the `tf.estimator` and `tf.layers` modules. Layers provide a level of abstraction over the raw operations between tensors. You can add easily regularization parameters, dropout layers, change the activation function, etc. The Estimator model, on the other hand, is a simple way of stacking the layers together. It also helps to divide the training, evaluation and prediction operations using the same model.\n",
    "\n",
    "However, even if these modules use a higher level abstraction, they still allow for a full customization and access to the low level variables.\n",
    "\n",
    "## The model architecture\n",
    "\n",
    "Now we can start creating and connecting the layers of the model in the correct order. The input for this function is one batch of the matrix with the representation of the data, created by the function `train_input_fn` defined above.\n",
    "\n",
    "For simplicity we will add now all the layers before the activation function of the last layer. The following model has two hidden layers, followed by a dropout layer and finally the output layer. The `tf.layers.dense` function needs to get as parameters the output of the previous layer and the size of the new output (units). It alse allows us to set several parameters for the layer, like the regularization, activation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_data, mode):\n",
    "    \"\"\"Creates the model layers.\n",
    "    \n",
    "    Args:\n",
    "        input_data: a Tensor with shape [batch_size, feature_size]\n",
    "    \n",
    "    Returns:\n",
    "        The logits of the output layer.\"\"\"\n",
    "    # Dense Layer #1\n",
    "    # Input Tensor Shape: [batch_size, embedding_size]\n",
    "    # Output Tensor Shape: [batch_size, hidden_layer_size_1]\n",
    "    hidden1 = tf.layers.dense(\n",
    "        inputs=input_data,\n",
    "        units=250,\n",
    "        activation=tf.nn.relu,\n",
    "        name='hidden_layer_1'\n",
    "    )\n",
    "\n",
    "    # Dense Layer #2\n",
    "    # Input Tensor Shape: [batch_size, hidden_layer_size_1]\n",
    "    # Output Tensor Shape: [batch_size, hidden_layer_size_2]\n",
    "    hidden2 = tf.layers.dense(\n",
    "        inputs=hidden1,\n",
    "        units=100,\n",
    "        activation=tf.nn.relu,\n",
    "        name='hidden_layer_2'\n",
    "    )\n",
    "\n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "    # The dropout only is applied when the model is training. For prediction\n",
    "    # and evaluation, the whole input is used.\n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs=hidden2, rate=0.4, training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "\n",
    "    # Logits layer. No activation\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, num_classes]\n",
    "    logits = tf.layers.dense(inputs=dropout, units=num_classes)\n",
    "\n",
    "    return (logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The structure of an Estimator\n",
    "\n",
    "So far we have defined the layers of our model, but we still need to connect the input data, add the prediction, loss and optimization function. All these is defined into a a function `model_fn` that will create the complete model. Then, this function is passed to the `tf.estimator.Estimator` object, which is just a wrapper that uses the model for training or prediction.\n",
    "\n",
    "The `model_fn` function must return a different `tf.estimator.EstimatorSpec` instance for each possible mode: TRAIN, EVAL and PREDICT. Note that for each mode, the behaviour of the model is different:\n",
    "  * TRAIN: the model uses the input to generate a prediction of labels, then it takes the given prediction and the true labels to calculate the loss function. The optimizer algorithm uses minimizes the loss with a backward pass updating all the model parameters.\n",
    "  * EVAL: the model uses the input to generate a prediction of labels, then it takes the given prediction to calculate some evaluation metrics.\n",
    "  * PREDICT: the model uses the input to generate a prediction of labels and returns them as result.\n",
    "\n",
    "We use the `EstimatorSpec` to enclose all those operations for the `Estimator` object to run them through its methods `train()`, `evaluate()` and `predict()`.\n",
    "\n",
    "Note that the `model_fn` function must have the parameters `features`, `labels` and `mode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for MLP.\n",
    "    \n",
    "    Args:\n",
    "        features: a dictionary where the values are input tensors with shape\n",
    "            [batch_size, feature_size]\n",
    "        labels: a tensor with shape [batch_size]\n",
    "        mode: a constant, one of `tf.estimator.ModeKeys.`\n",
    "    \n",
    "    Returns:\n",
    "        An instance of ´tf.estimator.EstimatorSpec´.\n",
    "    \"\"\"\n",
    "    logits = build_model(features['input_data'], mode)\n",
    "\n",
    "    predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        'classes': tf.argmax(input=logits, axis=1),\n",
    "        # Add `softmax_tensor` to the graph. It is used for PREDICT.\n",
    "        'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
    "    }\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=num_classes)\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "      onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        'accuracy': tf.metrics.accuracy(labels=labels, predictions=predictions['classes'])\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training cicle\n",
    "\n",
    "Now that we have the function that build the model, we can create the training cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create the Estimator\n",
    "mlp_classifier = tf.estimator.Estimator(\n",
    "    model_fn=mlp_model_fn, model_dir=\"20news_mlp_model\")\n",
    "\n",
    "# Train the model\n",
    "mlp_classifier.train(\n",
    "    input_fn=dataset_input_fn('train'),\n",
    "    steps=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "As seen before, it is also quite easy to get the evaluation metrics defined in the model after traning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_results = mlp_classifier.evaluate(input_fn=dataset_input_fn('test'))\n",
    "print(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
