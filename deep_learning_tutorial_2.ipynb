{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buiding the MLP model\n",
    "\n",
    "Now the dataset has been processed, we can build the MLP model using the `tf.estimator` and `tf.layers` modules. Layers provide a level of abstraction over the raw operations between tensors. You can add easily regularization parameters, dropout layers, change the activation function, etc. The Estimator model, on the other hand, is a simple way of stacking the layers together. It also helps to divide the training, evaluation and prediction operations using the same model.\n",
    "\n",
    "However, even if these modules use a higher level abstraction, they still allow for a full customization and access to the low level variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data management\n",
    "\n",
    "Before creating the model, we need to specify what the input and output is going to be. In the previous notebook we converted the documents from text into a numeric matrix that can be fed to the network. We read the variable from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO read arrays\n",
    "input_data = input_data = np.random.random([200, 5])\n",
    "input_labels = np.random.randint(1, 20, 200)\n",
    "num_classes = 20  # TODO calculate this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, most optimization algorithms similar to Stochastic Gradient Descent need the data in small portions for optimization purposes. On top of that, the training cycle goes through the entire dataset several times (epochs) before converging to a good solution.\n",
    "\n",
    "Fortunately, Tensorflow has the solution to iterate over datasets several times in small batches. These function are called input functions, and they can take a numpy array or a pandas dataframe. It's worth noticing that, during the past updates, Tensorflow has been including more functions to transform the input data in batches handling enconding of categorical features, embeddings, etc, althoug we wont use those function here.\n",
    "\n",
    "We create our input function with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"input_data\": input_data},  # A dictionary mapping string to input tensors\n",
    "    y=input_labels,\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model architecture\n",
    "\n",
    "Now we can start creating and connecting the layers of the model in the correct order. The input for this function is one batch of the matrix with the representation of the data, created by the function `train_input_fn` defined above.\n",
    "\n",
    "For simplicity we will add now all the layers before the activation function of the last layer. The following model has two hidden layers, followed by a dropout layer and finally the output layer. The `tf.layers.dense` function needs to get as parameters the output of the previous layer and the size of the new output (units). It alse allows us to set several parameters for the layer, like the regularization, activation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_data, mode):\n",
    "    \"\"\"Creates the model layers.\n",
    "    \n",
    "    Args:\n",
    "        input_data: a Tensor with shape [batch_size, feature_size]\n",
    "    \n",
    "    Returns:\n",
    "        The logits of the output layer.\"\"\"\n",
    "    # Dense Layer #1\n",
    "    # Input Tensor Shape: [batch_size, embedding_size]\n",
    "    # Output Tensor Shape: [batch_size, hidden_layer_size_1]\n",
    "    hidden1 = tf.layers.dense(\n",
    "        inputs=input_data,\n",
    "        units=250,\n",
    "        activation=tf.nn.relu,\n",
    "        name='hidden_layer_1'\n",
    "    )\n",
    "\n",
    "    # Dense Layer #2\n",
    "    # Input Tensor Shape: [batch_size, hidden_layer_size_1]\n",
    "    # Output Tensor Shape: [batch_size, hidden_layer_size_2]\n",
    "    hidden2 = tf.layers.dense(\n",
    "        inputs=hidden1,\n",
    "        units=100,\n",
    "        activation=tf.nn.relu,\n",
    "        name='hidden_layer_2'\n",
    "    )\n",
    "\n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "    # The dropout only is applied when the model is training. For prediction\n",
    "    # and evaluation, the whole input is used.\n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs=hidden2, rate=0.4, training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "\n",
    "    # Logits layer. No activation\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, num_classes]\n",
    "    logits = tf.layers.dense(inputs=dropout, units=num_classes)\n",
    "\n",
    "    return (logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The structure of an Estimator\n",
    "\n",
    "So far we have defined the layers of our model, but we still need to connect the input data, add the prediction, loss and optimization function. All these is defined into a a function `model_fn` that will create the complete model. Then, this function is passed to the `tf.estimator.Estimator` object, which is just a wrapper that uses the model for training or prediction.\n",
    "\n",
    "The `model_fn` function must return a different `tf.estimator.EstimatorSpec` instance for each possible mode: TRAIN, EVAL and PREDICT. Note that for each mode, the behaviour of the model is different:\n",
    "  * TRAIN: the model uses the input to generate a prediction of labels, then it takes the given prediction and the true labels to calculate the loss function. The optimizer algorithm uses minimizes the loss with a backward pass updating all the model parameters.\n",
    "  * EVAL: the model uses the input to generate a prediction of labels, then it takes the given prediction to calculate some evaluation metrics.\n",
    "  * PREDICT: the model uses the input to generate a prediction of labels and returns them as result.\n",
    "\n",
    "We use the `EstimatorSpec` to enclose all those operations for the `Estimator` object to run them through its methods `train()`, `evaluate()` and `predict()`.\n",
    "\n",
    "Note that the `model_fn` function must have the parameters `features`, `labels` and `mode`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for MLP.\n",
    "    \n",
    "    Args:\n",
    "        features: a dictionary where the values are input tensors with shape\n",
    "            [batch_size, feature_size]\n",
    "        labels: a tensor with shape [batch_size]\n",
    "        mode: a constant, one of `tf.estimator.ModeKeys.`\n",
    "    \n",
    "    Returns:\n",
    "        An instance of ´tf.estimator.EstimatorSpec´.\n",
    "    \"\"\"\n",
    "    logits = build_model(features['input_data'], mode)\n",
    "\n",
    "    predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        'classes': tf.argmax(input=logits, axis=1),\n",
    "        # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "        # `logging_hook`.\n",
    "        'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
    "    }\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=num_classes)\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "      onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        'accuracy': tf.metrics.accuracy(labels=labels, predictions=predictions['classes'])\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training cicle\n",
    "\n",
    "Now that we have the function that build the model, we can create the training cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_tf_random_seed': 1, '_save_checkpoints_steps': None, '_session_config': None, '_save_summary_steps': 100, '_keep_checkpoint_max': 5, '_model_dir': '/tmp/20news_mlp_model', '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_secs': 600}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/20news_mlp_model/model.ckpt-10000\n",
      "INFO:tensorflow:Saving checkpoints for 10001 into /tmp/20news_mlp_model/model.ckpt.\n",
      "INFO:tensorflow:step = 10001, loss = 2.90066099167\n",
      "INFO:tensorflow:global_step/sec: 483.377\n",
      "INFO:tensorflow:step = 10101, loss = 2.8757390976 (0.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 562.62\n",
      "INFO:tensorflow:step = 10201, loss = 2.88417220116 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 609.618\n",
      "INFO:tensorflow:step = 10301, loss = 2.88351988792 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 614.63\n",
      "INFO:tensorflow:step = 10401, loss = 2.89325261116 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 605.7\n",
      "INFO:tensorflow:step = 10501, loss = 2.82780337334 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 598.439\n",
      "INFO:tensorflow:step = 10601, loss = 2.91210412979 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 598.278\n",
      "INFO:tensorflow:step = 10701, loss = 2.89271688461 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 642.613\n",
      "INFO:tensorflow:step = 10801, loss = 2.87599372864 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 649.869\n",
      "INFO:tensorflow:step = 10901, loss = 2.82168626785 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 625.001\n",
      "INFO:tensorflow:step = 11001, loss = 2.96557998657 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 622.379\n",
      "INFO:tensorflow:step = 11101, loss = 2.89181685448 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 643.773\n",
      "INFO:tensorflow:step = 11201, loss = 3.02536273003 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 620.146\n",
      "INFO:tensorflow:step = 11301, loss = 2.84842920303 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 623.112\n",
      "INFO:tensorflow:step = 11401, loss = 2.90693807602 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 645.165\n",
      "INFO:tensorflow:step = 11501, loss = 2.83217382431 (0.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 635.495\n",
      "INFO:tensorflow:step = 11601, loss = 2.89179730415 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 638.067\n",
      "INFO:tensorflow:step = 11701, loss = 2.90454816818 (0.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 616.074\n",
      "INFO:tensorflow:step = 11801, loss = 2.97054791451 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 611.596\n",
      "INFO:tensorflow:step = 11901, loss = 2.92280435562 (0.163 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 12000 into /tmp/20news_mlp_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.95835733414.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f77007340f0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Estimator\n",
    "mlp_classifier = tf.estimator.Estimator(\n",
    "    model_fn=mlp_model_fn, model_dir=\"/tmp/20news_mlp_model\")\n",
    "\n",
    "# Set up logging for predictions\n",
    "# Log the values in the \"Softmax\" tensor with label \"probabilities\"\n",
    "# tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "# logging_hook = tf.train.LoggingTensorHook(\n",
    "#     tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "# Train the model\n",
    "mlp_classifier.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=2000,\n",
    "#    hooks=[logging_hook]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "As seen before, it is also quite easy to get the evaluation metrics defined in the model after traning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-11-03-17:39:29\n",
      "INFO:tensorflow:Restoring parameters from /tmp/20news_mlp_model/model.ckpt-12000\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-03-17:39:30\n",
      "INFO:tensorflow:Saving dict for global step 12000: accuracy = 0.065, global_step = 12000, loss = 2.97732\n",
      "{'accuracy': 0.064999998, 'loss': 2.9773197, 'global_step': 12000}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and print results\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"input_data\": input_data},\n",
    "    y=input_labels,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "eval_results = mlp_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
